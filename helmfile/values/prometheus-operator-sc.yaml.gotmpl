global:
  rbac:
    pspEnabled: {{ requiredEnv "ENABLE_PSP" }}

kube-state-metrics:
  podSecurityPolicy:
    enabled: {{ requiredEnv "ENABLE_PSP" }}

kubeApiServer:
  serviceMonitor:
    metricRelabelings:
      - action: keep
        sourceLabels: [__name__]
        regex: '(apiserver_request_count|apiserver_request_latencies_bucket|apiserver_client_certificate_expiration_seconds_count|)'

alertmanager:
  config:
    # See https://prometheus.io/docs/alerting/configuration/
    route:
      group_by: [cluster, alertname]
      # default receiver
      receiver: '{{ requiredEnv "ALERT_TO" }}'
      routes:
      - match:
          # The watchdog alert is always active, send it to null.
          alertname: Watchdog
        receiver: 'null'
      - match:
          # TODO: it would be nice to do this some other way.
          # The workload_cluster does not have an alertmanager, send to null.
          alertname: AlertmanagerDown
          cluster: workload_cluster
        receiver: 'null'
      - match:
          # TODO: it would be nice to do this some other way.
          # The workload_cluster does not have an alertmanager, send to null.
          alertname: PrometheusNotConnectedToAlertmanagers
          cluster: workload_cluster
        receiver: 'null'
      - match:
          # This alert is a bit over sensitive at the moment
          # See this: https://github.com/kubernetes-monitoring/kubernetes-mixin/issues/108
          alertname: CPUThrottlingHigh
        receiver: 'null'
      - match:
          # This alert is a bit over sensitive at the moment (always active for a cluster with one worker node), send to null.
          alertname: KubeCPUOvercommit
        receiver: 'null'
      - match:
          # This alert is a bit over sensitive at the moment (always active for a cluster with one worker node), send to null.
          alertname: KubeMemOvercommit
        receiver: 'null'
    receivers:
    - name: 'null'
    - name: slack
      {{ if eq (env "ALERT_TO") "slack" }}
      slack_configs:
      # Note: the channel here does not apply if the webhook URL is for a specific channel
      - channel: techteam
        # Webhook URL for slack, see https://api.slack.com/apps/ANJ11SFK3/general?
        api_url: {{ requiredEnv "SLACK_API_URL" }}
        send_resolved: true
        # Alertmanager templating: https://prometheus.io/docs/alerting/notifications/
        # We need to escape the templating brackets for alertmanager here with
        # {{``}} to prevent helm from parsing them.
        # See: https://github.com/helm/helm/issues/2798#issuecomment-467319526
        text: |-
          <!channel>
          *Cluster:* grafana.{{ requiredEnv "ECK_OPS_DOMAIN" }}
          {{`
          *Common summary:* {{ .CommonAnnotations.summary }}
          *Common description:* {{ .CommonAnnotations.description }}
          {{ range .CommonLabels.SortedPairs }}
          *{{ .Name }}:* {{ .Value }}
          {{ end }}

          *Individual alerts below*
          {{ range .Alerts }}
          *Status:* {{ .Status }}
          {{ range .Annotations.SortedPairs }}
          *{{ .Name }}:* {{ .Value }}
          {{ end }}
          {{ end }}`}}
      {{ end }}

# TODO: These do not work out of the box with RKE
# If we switch to kubeadm they should be fine, otherwise we should look into
# how to configure them properly for RKE.
kubeControllerManager:
  enabled: false
kubeScheduler:
  enabled: false

grafana:
  # admin username is "admin"
  adminPassword: {{ requiredEnv "GRAFANA_PWD" }}

  ingress:
    enabled: true
    annotations:
      ingress.kubernetes.io/rewrite-target: /
      certmanager.k8s.io/issuer: letsencrypt-{{ requiredEnv "CERT_TYPE" }}
    hosts:
    - grafana.{{ requiredEnv "ECK_OPS_DOMAIN" }}
    ## TLS configuration for grafana Ingress
    ## Secret must be manually created in the namespace
    tls:
    - secretName: grafana-general-tls
      hosts:
      - grafana.{{ requiredEnv "ECK_OPS_DOMAIN" }}
  rbac:
    pspUseAppArmor: false
    pspEnabled: {{ requiredEnv "ENABLE_PSP" }}

  persistence:
    type: pvc
    enabled: true
    # Specify a storage class to use or comment out to use the default
    # storageClassName:
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  serviceAccount:
    create: true
    name:
    nameTest:

  sidecar:
    datasources:
      enabled: true
      defaultDatasourceEnabled: false

  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default

  dashboards:
    default: {}

  additionalDataSources:
  - name: prometheus-wc-reader
    access: proxy
    basicAuth: false
    editable: false
    isDefault: false
    orgId: 1
    type: prometheus
    url: http://wc-scraper-prometheus-instance:9090
    version: 1
  - name: prometheus-ss
    access: proxy
    basicAuth: false
    editable: false
    isDefault: false
    orgId: 1
    type: prometheus
    url: http://prometheus-operator-prometheus:9090
    version: 1

  grafana.ini:
    server:
      root_url: https://grafana.{{ requiredEnv "ECK_OPS_DOMAIN" }}
    auth.generic_oauth:
      name: dex
      enabled: true
      client_id: grafana
      client_secret: {{ requiredEnv "GRAFANA_CLIENT_SECRET" }}
      scopes: profile email openid
      auth_url: https://dex.{{ requiredEnv "ECK_BASE_DOMAIN" }}/auth
      token_url: https://dex.{{ requiredEnv "ECK_BASE_DOMAIN" }}/token
      api_url: https://dex.{{ requiredEnv "ECK_BASE_DOMAIN" }}/api
      allowed_domains: {{ requiredEnv "OAUTH_ALLOWED_DOMAINS" }}
      allow_sign_up: true
      tls_skip_verify_insecure: {{ requiredEnv "TLS_SKIP_VERIFY" }}

prometheusOperator:
  createCustomResource: false

defaultRules:
  labels:
    role: operator

prometheus:
  prometheusSpec:
    resources:
      requests:
        memory: 1Gi # Total will be 50Mi more, acounting for prometheus-config-reloader & prometheus-rulefiles
        cpu: "300m" # Total will be 200m more, acounting for prometheus-config-reloader & prometheus-rulefiles
      limits:
        memory: 2Gi # Total will be 50Mi more, acounting for prometheus-config-reloader & prometheus-rulefiles
        cpu: 1 # Total will be 200m more, acounting for prometheus-config-reloader & prometheus-rulefiles
    remoteRead:
    - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/read?db=service_cluster&u=admin&p={{ requiredEnv "INFLUXDB_PWD" }}
    remoteWrite:
    - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/write?db=service_cluster&u=admin&p={{ requiredEnv "INFLUXDB_PWD" }}
    externalLabels:
      cluster: service_cluster
    ruleSelector:
      matchLabels:
        role: operator
