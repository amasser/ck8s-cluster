prometheusSpec:
  version: "v2.19.2"
  alerting:
    # Use the default alertmanager that comes with prometheus-operator
    alertmanagers:
    - name: prometheus-operator-alertmanager
      namespace: monitoring
      pathPrefix: /
      port: web
  resources:
    requests:
      memory: "1Gi"
      cpu: "0.3"
    limits:
      memory: "2Gi"
      cpu: "1"

    ## How long to retain metrics
    ##
  retention: {{ requiredEnv "PROMETHEUS_RETENTION_WC" }}

  ## Maximum size of metrics
  ##
  retentionSize: {{ requiredEnv "PROMETHEUS_RETENTION_SIZE_WC" }}

  ## Prometheus StorageSpec for persistent data
  ## ref: https://github.com/coreos/prometheus-operator/blob/release-0.29/Documentation/user-guides/storage.md
  ##
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: {{ requiredEnv "STORAGE_CLASS" }}
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: {{ requiredEnv "PROMETHEUS_STORAGE_SIZE_WC" }}

  enableAdminAPI: false
  logFormat: logfmt
  logLevel: info
  remoteRead:
  - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/read?db=workload_cluster&u=admin&p={{ requiredEnv "INFLUXDB_PWD"}}
  remoteWrite:
  - url: http://influxdb.influxdb-prometheus.svc.cluster.local:8086/api/v1/prom/write?db=workload_cluster&u=admin&p={{ requiredEnv "INFLUXDB_PWD"}}
  replicas: 1
  retention: 10d
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-operator-prometheus
  serviceMonitorSelector:
    matchLabels:
      target: none  # so that operator wont generate new config
  ruleNamespaceSelector: {}
  ruleSelector:
    matchLabels:
      cluster: workload

  externalLabels:
    scraper: wc-scraper

  # Don't add prometheus info.
  prometheusExternalLabelName: ""
  replicaExternalLabelName: ""

scrapeConfig:
#The idea with splitting the federation in two is to (hopefully) reduce the resources needed for prometheus.
#If I have understood it correctly prometheus will use more resources (primarily memory) if you have larger scrapes.
#So if we scrape everything from another prometheus it will be a large job where prometheus might run out of memory.
#But splitting them might reduce the memory needed.
#The kubelet is roughly half of the metrics, so these two scrapes will be roughly the same size.
- job_name: 'federate-kubelet'
  scrape_interval: 30s

  honor_labels: true
  metrics_path: '/federate'

  scheme: https

  params:
    'match[]':
      - '{job="kubelet"}'
  basic_auth:
    username: prometheus
    password: {{ requiredEnv "CUSTOMER_PROMETHEUS_PWD"}}

  tls_config:
    insecure_skip_verify: {{ requiredEnv "TLS_SKIP_VERIFY"}}

  static_configs:
    - targets:
      - prometheus.{{ requiredEnv "ECK_BASE_DOMAIN"}}

- job_name: 'federate-others'
  scrape_interval: 30s

  honor_labels: true
  metrics_path: '/federate'

  scheme: https

  params:
    'match[]':
      # Match all non empty job labels except "kubelet"
      - '{job!="kubelet", job=~".+"}'
  basic_auth:
    username: prometheus
    password: {{ requiredEnv "CUSTOMER_PROMETHEUS_PWD"}}

  tls_config:
    insecure_skip_verify: {{ requiredEnv "TLS_SKIP_VERIFY"}}

  static_configs:
    - targets:
      - prometheus.{{ requiredEnv "ECK_BASE_DOMAIN"}}
